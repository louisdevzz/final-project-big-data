# Airflow Big Data Pipeline System

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airflow Scheduler                         â”‚
â”‚              (Quáº£n lÃ½ DAGs vÃ  láº­p lá»‹ch tasks)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Redis Broker                              â”‚
â”‚              (Message queue cho Celery tasks)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚               â”‚              â”‚
        â–¼              â–¼               â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker  â”‚   â”‚  Worker  â”‚   â”‚  Worker  â”‚   â”‚  Worker  â”‚
â”‚  Spark   â”‚   â”‚  Spark   â”‚   â”‚  Hadoop  â”‚   â”‚  Hadoop  â”‚
â”‚  Master  â”‚   â”‚  Worker  â”‚   â”‚Namenode  â”‚   â”‚Datanode  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚               â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PostgreSQL Database                       â”‚
â”‚          (LÆ°u metadata Airflow & káº¿t quáº£ Celery)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone vÃ  chuáº©n bá»‹ mÃ´i trÆ°á»ng

```bash
cd ~/Documents/airflow/airflow
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

Hoáº·c sá»­ dá»¥ng `uv` (khuyáº¿n nghá»‹):

```bash
uv sync
source .venv/bin/activate
```

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng Airflow cluster

```bash
# Táº¡o file .env náº¿u chÆ°a cÃ³
echo "AIRFLOW_UID=$(id -u)" > .env

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker compose ps
```

Airflow Web UI sáº½ cÃ³ sáºµn táº¡i: `http://localhost:9090`

- Username: `airflow`
- Password: `airflow`

### BÆ°á»›c 4: Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng

Cáº­p nháº­t cÃ¡c biáº¿n mÃ´i trÆ°á»ng trong `docker-compose.yaml` hoáº·c `.env`:

```bash
# Redis broker (Ä‘iá»u chá»‰nh IP theo mÃ´i trÆ°á»ng cá»§a báº¡n)
AIRFLOW__CELERY__BROKER_URL=redis://:@192.168.80.229:6379/0

# PostgreSQL database
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
```

---

## ğŸ”§ Cáº¥u hÃ¬nh Worker

### Triá»ƒn khai Workers

#### BÆ°á»›c 1: Deploy config files tá»›i workers

**CÃ¡ch 1: Automated Deployment (Khuyáº¿n nghá»‹ - Deploy táº¥t cáº£ cÃ¹ng lÃºc)**

Sá»­ dá»¥ng script `deploy_worker_config.sh` Ä‘á»ƒ deploy config tá»± Ä‘á»™ng tá»« mÃ¡y trung tÃ¢m Ä‘áº¿n táº¥t cáº£ workers:

```bash
cd ~/Documents/airflow/airflow

# Cáº¥u hÃ¬nh mapping trong scripts/deploy_worker_config.sh
# HOSTS: Danh sÃ¡ch IP cá»§a workers
# CONFIGS: Config file tÆ°Æ¡ng á»©ng vá»›i má»—i worker
# SSH_USER: Username Ä‘á»ƒ SSH (máº·c Ä‘á»‹nh: donghuynh0)

# Deploy tá»›i táº¥t cáº£ workers
./scripts/deploy_worker_config.sh
```

Script sáº½:

- âœ… Hiá»ƒn thá»‹ deployment plan (host â†’ config mapping)
- âœ… Copy config files qua SSH tá»›i tá»«ng mÃ¡y
- âœ… Táº¡o thÆ° má»¥c `/etc/celery/` vá»›i sudo
- âœ… Verify deployment thÃ nh cÃ´ng
- âœ… HÆ°á»›ng dáº«n next steps

**CÃ¡ch 2: Manual Deployment (Tá»«ng mÃ¡y má»™t)**

Náº¿u muá»‘n deploy thá»§ cÃ´ng trÃªn tá»«ng mÃ¡y:

```bash
# TrÃªn mÃ¡y worker
sudo mkdir -p /etc/celery

# Copy config tÆ°Æ¡ng á»©ng vá»›i vai trÃ² cá»§a mÃ¡y
# VÃ­ dá»¥: MÃ¡y Spark Master
sudo cp worker_configs/worker_capabilities.spark_master.json /etc/celery/worker_capabilities.json
```

**CÃ¡c file config máº«u:**

- `worker_capabilities.spark_master.json` - Cho Spark Master
- `worker_capabilities.spark_worker.json` - Cho Spark Worker
- `worker_capabilities.hadoop_namenode.json` - Cho Hadoop Namenode
- `worker_capabilities.hadoop_datanode.json` - Cho Hadoop Datanode

**VÃ­ dá»¥ ná»™i dung config:**

```json
{
  "worker_name": "spark-master-worker",
  "capabilities": [
    "spark_master",
    "docker_host",
    "prepare_data",
    "train_model",
    "streaming_data",
    "predict"
  ],
  "description": "Spark Master node - handles master services and data pipeline tasks"
}
```

> ğŸ’¡ **Tip**: Sá»­ dá»¥ng automated deployment khi cÃ³ nhiá»u workers Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian vÃ  trÃ¡nh sai sÃ³t. Chá»‰ cáº§n cáº¥u hÃ¬nh mapping trong `deploy_worker_config.sh` má»™t láº§n, sau Ä‘Ã³ cÃ³ thá»ƒ re-deploy dá»… dÃ ng khi cáº§n update config.

#### BÆ°á»›c 2: Start worker

**CÃ¡ch 1: Sá»­ dá»¥ng script tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)**

```bash
cd ~/Documents/airflow/airflow
./scripts/start_worker.sh
```

Script sáº½:

- âœ… Tá»± Ä‘á»™ng detect capabilities tá»« config file
- âœ… Kiá»ƒm tra káº¿t ná»‘i Redis
- âœ… Subscribe vÃ o cÃ¡c queues phÃ¹ há»£p
- âœ… Start worker vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u

**CÃ¡ch 2: Manual start**

```bash
# Worker tá»± Ä‘á»™ng load config tá»« /etc/celery/worker_capabilities.json
celery -A mycelery.system_worker worker --loglevel=info

# Hoáº·c chá»‰ Ä‘á»‹nh queues thá»§ cÃ´ng
celery -A mycelery.system_worker worker \
  --queues=spark_master,docker_host,prepare_data,train_model \
  --loglevel=info \
  --concurrency=4 \
  --hostname=worker-spark-master@%h
```

#### BÆ°á»›c 3: Verify workers

```bash
# Kiá»ƒm tra workers Ä‘ang hoáº¡t Ä‘á»™ng
celery -A mycelery.system_worker inspect active

# Kiá»ƒm tra queues mÃ  worker subscribe
celery -A mycelery.system_worker inspect active_queues

# Kiá»ƒm tra stats
celery -A mycelery.system_worker inspect stats
```

#### Cáº¥u hÃ¬nh Deployment Script

Äá»ƒ sá»­ dá»¥ng automated deployment, cáº§n cáº¥u hÃ¬nh mapping trong `scripts/deploy_worker_config.sh`:

```bash
# Chá»‰nh sá»­a file scripts/deploy_worker_config.sh

# 1. Cáº­p nháº­t danh sÃ¡ch hosts (IP cá»§a workers)
HOSTS=(
    "192.168.80.55"   # Spark Master
    "192.168.80.53"   # Spark Worker
    "192.168.80.57"   # Hadoop Namenode + Kafka
    "192.168.80.87"   # Hadoop Datanode
)

# 2. Config files tÆ°Æ¡ng á»©ng
CONFIGS=(
    "worker_capabilities.spark_master.json"
    "worker_capabilities.spark_worker.json"
    "worker_capabilities.hadoop_namenode.json"
    "worker_capabilities.hadoop_datanode.json"
)

# 3. SSH username (hoáº·c set env var)
SSH_USER="${SSH_USER:-donghuynh0}"  # Thay báº±ng username cá»§a báº¡n
```

**LÆ°u Ã½:**

- âœ… Thá»© tá»± trong `HOSTS` pháº£i khá»›p vá»›i thá»© tá»± trong `CONFIGS`
- âœ… User cáº§n cÃ³ quyá»n sudo trÃªn cÃ¡c mÃ¡y workers
- âœ… Setup SSH key-based authentication Ä‘á»ƒ trÃ¡nh nháº­p password nhiá»u láº§n:
  ```bash
  # TrÃªn mÃ¡y trung tÃ¢m
  ssh-copy-id user@worker-host
  ```

### Cáº¥u hÃ¬nh nÃ¢ng cao

Worker settings cÃ³ thá»ƒ Ä‘iá»u chá»‰nh qua environment variables:

```bash
export CELERY_WORKER_CONCURRENCY=4          # Sá»‘ tasks cháº¡y Ä‘á»“ng thá»i
export CELERY_WORKER_MAX_TASKS_PER_CHILD=100  # Restart sau N tasks
export CELERY_WORKER_TIME_LIMIT=3600        # Timeout (seconds)
export CELERY_WORKER_LOG_LEVEL=info         # Log level
```

---

## ğŸ”„ Luá»“ng hoáº¡t Ä‘á»™ng

### 1. Kiáº¿n trÃºc thá»±c thi

```
Airflow Scheduler
    â”‚
    â”œâ”€â–º DAG: bigdata_pipeline_start
    â”‚      â”‚
    â”‚      â”œâ”€â–º [PythonOperator] start_hadoop_namenode
    â”‚      â”‚        â””â”€â–º Celery Task â†’ Queue: hadoop_namenode
    â”‚      â”‚                 â””â”€â–º Worker (capability: hadoop_namenode)
    â”‚      â”‚                      â””â”€â–º docker_compose_up(~/bd/hadoop/...)
    â”‚      â”‚
    â”‚      â”œâ”€â–º [PythonOperator] start_spark_master
    â”‚      â”‚        â””â”€â–º Celery Task â†’ Queue: spark_master
    â”‚      â”‚                 â””â”€â–º Worker (capability: spark_master)
    â”‚      â”‚
    â”‚      â””â”€â–º [PythonOperator] prepare_data
    â”‚               â””â”€â–º Celery Task â†’ Queue: prepare_data
    â”‚                        â””â”€â–º Worker (capability: prepare_data)
    â”‚                             â””â”€â–º run_command(sh prepare.sh)
    â”‚
    â””â”€â–º DAG: bigdata_pipeline_stop
           â””â”€â–º [PythonOperator] stop services...
```

### 2. Task Routing Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow Task Submit     â”‚
â”‚  op='start_spark_master' â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Task Created     â”‚
â”‚  queue='spark_master'    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Broker            â”‚
â”‚  Add to queue list       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker Selection        â”‚
â”‚  Find workers with       â”‚
â”‚  'spark_master' cap      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Task Execution          â”‚
â”‚  docker_compose_up(...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Result to PostgreSQL    â”‚
â”‚  Airflow tracks status   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3. Configuration Files

#### **DAG Configuration** (`dags/system_control_dag.py`)

Äá»‹nh nghÄ©a services vÃ  mapping capabilities:

```python
BIGDATA_SERVICES = {
    'spark-master': {
        'capability': 'spark_master',
        'path': '~/bd/spark/docker-compose.yml',
        'service': 'spark-master',
    },
    'hadoop-namenode': {
        'capability': 'hadoop_namenode',
        'path': '~/bd/hadoop/docker-compose.namenode.yml',
    },
    # ...
}
```

#### **Worker Configuration** (`mycelery/worker_config.py`)

Map capabilities sang queues:

```python
queue_mapping = {
    'spark_master': ['spark_master', 'spark_common'],
    'prepare_data': ['prepare_data'],
    'train_model': ['train_model'],
    # ...
}
```

#### **Celery Configuration** (`mycelery/celeryconfig.py`)

```python
broker_url = 'redis://192.168.80.229:6379/0'
result_backend = 'db+postgresql://airflow:airflow@192.168.80.229/airflow'
task_time_limit = 3600
worker_concurrency = 4
```

---

## ğŸ” Troubleshooting

### 1. Worker khÃ´ng nháº­n tasks

**Triá»‡u chá»©ng**: Tasks stuck á»Ÿ tráº¡ng thÃ¡i "queued"

**Kiá»ƒm tra:**

```bash
# 1. Worker cÃ³ Ä‘ang cháº¡y khÃ´ng?
celery -A mycelery.system_worker inspect active_queues

# 2. Config file cÃ³ Ä‘Ãºng khÃ´ng?
cat /etc/celery/worker_capabilities.json

# 3. Redis connection
redis-cli -h 192.168.80.229 ping
```

**Giáº£i phÃ¡p:**

```bash
# Restart worker vá»›i verbose logging
celery -A mycelery.system_worker worker \
  --loglevel=debug \
  --queues=spark_master
```

### 2. Task timeout

**Triá»‡u chá»©ng**: Task bÃ¡o lá»—i "TimeLimitExceeded"

**TÄƒng timeout:**

```python
# Trong DAG
result = run_command.apply_async(
    args=[command],
    kwargs={'env_vars': env_vars},
    queue='train_model',
    time_limit=7200  # 2 hours
)
```

Hoáº·c cáº¥u hÃ¬nh global trong `celeryconfig.py`:

```python
task_time_limit = 7200
task_soft_time_limit = 6600
```

### 3. Airflow khÃ´ng tháº¥y DAGs

**Kiá»ƒm tra:**

```bash
# 1. Xem logs cá»§a dag-processor
docker compose logs airflow-dag-processor

# 2. Verify PYTHONPATH
docker compose exec airflow-scheduler printenv PYTHONPATH
# Should contain: /opt/airflow:/opt/airflow/dags:/opt/airflow/mycelery

# 3. Check DAG syntax
docker compose exec airflow-scheduler \
  python /opt/airflow/dags/system_control_dag.py
```

### 4. Redis connection error

**Lá»—i**: "Error connecting to Redis"

**Kiá»ƒm tra:**

```bash
# 1. Redis cÃ³ cháº¡y khÃ´ng?
docker compose ps redis

# 2. Test connection
redis-cli -h 192.168.80.229 -p 6379 ping

# 3. Check firewall
telnet 192.168.80.229 6379
```

**Cáº­p nháº­t broker URL:**

```bash
# Trong docker-compose.yaml
AIRFLOW__CELERY__BROKER_URL: redis://:@<IP_Má»šI>:6379/0
```

### 5. PostgreSQL connection error

**Kiá»ƒm tra:**

```bash
# Test connection
docker compose exec postgres psql -U airflow -d airflow -c "SELECT 1;"

# Xem logs
docker compose logs postgres
```

### 6. Worker khÃ´ng load capabilities

**Triá»‡u chá»©ng**: Worker chá»‰ subscribe vÃ o queue "celery"

**Debug:**

```bash
# Test worker config
python3 -c "
from mycelery.worker_config import get_worker_capabilities, get_worker_queues
print('Capabilities:', get_worker_capabilities())
print('Queues:', get_worker_queues())
"

# Verify file exists
ls -la /etc/celery/worker_capabilities.json

# Check permissions
sudo chmod 644 /etc/celery/worker_capabilities.json
```

### 7. Docker Compose command fails

**Lá»—i trong logs**: "docker compose up failed"

**Kiá»ƒm tra:**

```bash
# Test command manually trÃªn worker
docker compose -f ~/bd/spark/docker-compose.yml ps

# Verify path exists
ls -la ~/bd/spark/docker-compose.yml

# Check Docker installation
docker --version
docker compose version
```
