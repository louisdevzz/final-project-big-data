# Airflow Big Data Pipeline System

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#giá»›i-thiá»‡u)
- [Kiáº¿n trÃºc há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [Cáº¥u hÃ¬nh Worker](#cáº¥u-hÃ¬nh-worker)
- [Luá»“ng hoáº¡t Ä‘á»™ng](#luá»“ng-hoáº¡t-Ä‘á»™ng)
- [Sá»­ dá»¥ng](#sá»­-dá»¥ng)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Giá»›i thiá»‡u

Há»‡ thá»‘ng quáº£n lÃ½ Big Data Pipeline sá»­ dá»¥ng **Apache Airflow** vÃ  **Celery** Ä‘á»ƒ Ä‘iá»u phá»‘i cÃ¡c tÃ¡c vá»¥ phÃ¢n tÃ¡n trÃªn nhiá»u mÃ¡y chá»§. Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ theo mÃ´ hÃ¬nh **capability-based routing**, cho phÃ©p linh hoáº¡t trong viá»‡c phÃ¢n bá»• tÃ¡c vá»¥ mÃ  khÃ´ng cáº§n hardcode IP hoáº·c hostname cá»§a tá»«ng mÃ¡y.

### âœ¨ TÃ­nh nÄƒng chÃ­nh

- âœ… **Capability-based routing**: Tasks Ä‘Æ°á»£c Ä‘á»‹nh tuyáº¿n dá»±a trÃªn kháº£ nÄƒng cá»§a worker
- âœ… **Distributed execution**: Cháº¡y tasks song song trÃªn nhiá»u mÃ¡y
- âœ… **Docker orchestration**: Quáº£n lÃ½ cÃ¡c dá»‹ch vá»¥ Big Data qua Docker Compose
- âœ… **Data pipeline automation**: Tá»± Ä‘á»™ng hÃ³a quy trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u vá»›i Spark, Hadoop, Kafka
- âœ… **High availability**: Há»— trá»£ nhiá»u workers cÃ¹ng capability cho load balancing vÃ  failover

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

### SÆ¡ Ä‘á»“ tá»•ng quan

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Airflow Scheduler                         â”‚
â”‚              (Quáº£n lÃ½ DAGs vÃ  láº­p lá»‹ch tasks)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Redis Broker                              â”‚
â”‚              (Message queue cho Celery tasks)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚              â”‚               â”‚              â”‚
        â–¼              â–¼               â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker  â”‚   â”‚  Worker  â”‚   â”‚  Worker  â”‚   â”‚  Worker  â”‚
â”‚  Spark   â”‚   â”‚  Spark   â”‚   â”‚  Hadoop  â”‚   â”‚  Hadoop  â”‚
â”‚  Master  â”‚   â”‚  Worker  â”‚   â”‚Namenode  â”‚   â”‚Datanode  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚              â”‚               â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PostgreSQL Database                       â”‚
â”‚          (LÆ°u metadata Airflow & káº¿t quáº£ Celery)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c thÃ nh pháº§n

#### 1. **Airflow Components** (Docker containers)

| Service                 | Port | MÃ´ táº£                                                 |
| ----------------------- | ---- | ----------------------------------------------------- |
| `postgres`              | 5432 | CÆ¡ sá»Ÿ dá»¯ liá»‡u PostgreSQL lÆ°u metadata vÃ  task results |
| `redis`                 | 6379 | Message broker cho Celery                             |
| `airflow-apiserver`     | 9090 | API server cá»§a Airflow 3.0+                           |
| `airflow-scheduler`     | -    | Láº­p lá»‹ch vÃ  trigger cÃ¡c DAGs                          |
| `airflow-dag-processor` | -    | Xá»­ lÃ½ DAG files                                       |
| `airflow-worker`        | -    | Celery worker máº·c Ä‘á»‹nh (default queue)                |
| `airflow-triggerer`     | -    | Xá»­ lÃ½ deferred tasks                                  |

#### 2. **Celery Workers** (Cháº¡y trÃªn cÃ¡c mÃ¡y riÃªng biá»‡t)

Workers Ä‘Æ°á»£c phÃ¢n loáº¡i theo **capabilities** thay vÃ¬ hardcode IP:

| Capability        | Queue             | Chá»©c nÄƒng                    |
| ----------------- | ----------------- | ---------------------------- |
| `spark_master`    | `spark_master`    | Cháº¡y spark-submit commands   |
| `spark_worker`    | `spark_worker`    | Distributed Spark processing |
| `hadoop_namenode` | `hadoop_namenode` | Quáº£n lÃ½ HDFS metadata        |
| `hadoop_datanode` | `hadoop_datanode` | LÆ°u trá»¯ HDFS data            |
| `kafka`           | `kafka`           | Kafka broker services        |
| `docker_host`     | `docker_host`     | Báº¥t ká»³ mÃ¡y cÃ³ Docker         |
| `prepare_data`    | `prepare_data`    | Data preparation task        |
| `train_model`     | `train_model`     | Model training task          |
| `streaming_data`  | `streaming_data`  | Real-time streaming task     |
| `predict`         | `predict`         | Prediction/inference task    |

#### 3. **Big Data Services**

CÃ¡c services Ä‘Æ°á»£c quáº£n lÃ½ qua Docker Compose:

- **Spark**: Master + Workers
- **Hadoop**: Namenode + Datanodes
- **Kafka**: Broker + Zookeeper

---

## âš™ï¸ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Clone vÃ  chuáº©n bá»‹ mÃ´i trÆ°á»ng

```bash
cd ~/Documents/airflow/airflow
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

Hoáº·c sá»­ dá»¥ng `uv` (khuyáº¿n nghá»‹):

```bash
uv sync
source .venv/bin/activate
```

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng Airflow cluster

```bash
# Táº¡o file .env náº¿u chÆ°a cÃ³
echo "AIRFLOW_UID=$(id -u)" > .env

# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker compose up -d

# Kiá»ƒm tra tráº¡ng thÃ¡i
docker compose ps
```

Airflow Web UI sáº½ cÃ³ sáºµn táº¡i: `http://localhost:9090`

- Username: `airflow`
- Password: `airflow`

### BÆ°á»›c 4: Cáº¥u hÃ¬nh mÃ´i trÆ°á»ng

Cáº­p nháº­t cÃ¡c biáº¿n mÃ´i trÆ°á»ng trong `docker-compose.yaml` hoáº·c `.env`:

```bash
# Redis broker (Ä‘iá»u chá»‰nh IP theo mÃ´i trÆ°á»ng cá»§a báº¡n)
AIRFLOW__CELERY__BROKER_URL=redis://:@192.168.80.229:6379/0

# PostgreSQL database
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
```

---

## ğŸ”§ Cáº¥u hÃ¬nh Worker

### Capability-based Routing

Há»‡ thá»‘ng sá»­ dá»¥ng mÃ´ hÃ¬nh **capability-based routing** Ä‘á»ƒ phÃ¢n phá»‘i tasks linh hoáº¡t:

**Lá»£i Ã­ch:**

- âœ… IP thay Ä‘á»•i khÃ´ng áº£nh hÆ°á»Ÿng há»‡ thá»‘ng
- âœ… Dá»… dÃ ng scale workers (thÃªm/bá»›t mÃ¡y)
- âœ… Tá»± Ä‘á»™ng load balancing
- âœ… High availability (nhiá»u workers cÃ¹ng capability)

### Triá»ƒn khai Workers

#### BÆ°á»›c 1: Deploy config files tá»›i workers

**CÃ¡ch 1: Automated Deployment (Khuyáº¿n nghá»‹ - Deploy táº¥t cáº£ cÃ¹ng lÃºc)**

Sá»­ dá»¥ng script `deploy_worker_config.sh` Ä‘á»ƒ deploy config tá»± Ä‘á»™ng tá»« mÃ¡y trung tÃ¢m Ä‘áº¿n táº¥t cáº£ workers:

```bash
cd ~/Documents/airflow/airflow

# Cáº¥u hÃ¬nh mapping trong scripts/deploy_worker_config.sh
# HOSTS: Danh sÃ¡ch IP cá»§a workers
# CONFIGS: Config file tÆ°Æ¡ng á»©ng vá»›i má»—i worker
# SSH_USER: Username Ä‘á»ƒ SSH (máº·c Ä‘á»‹nh: donghuynh0)

# Deploy tá»›i táº¥t cáº£ workers
./scripts/deploy_worker_config.sh
```

Script sáº½:

- âœ… Hiá»ƒn thá»‹ deployment plan (host â†’ config mapping)
- âœ… Copy config files qua SSH tá»›i tá»«ng mÃ¡y
- âœ… Táº¡o thÆ° má»¥c `/etc/celery/` vá»›i sudo
- âœ… Verify deployment thÃ nh cÃ´ng
- âœ… HÆ°á»›ng dáº«n next steps

**CÃ¡ch 2: Manual Deployment (Tá»«ng mÃ¡y má»™t)**

Náº¿u muá»‘n deploy thá»§ cÃ´ng trÃªn tá»«ng mÃ¡y:

```bash
# TrÃªn mÃ¡y worker
sudo mkdir -p /etc/celery

# Copy config tÆ°Æ¡ng á»©ng vá»›i vai trÃ² cá»§a mÃ¡y
# VÃ­ dá»¥: MÃ¡y Spark Master
sudo cp worker_configs/worker_capabilities.spark_master.json /etc/celery/worker_capabilities.json
```

**CÃ¡c file config máº«u:**

- `worker_capabilities.spark_master.json` - Cho Spark Master
- `worker_capabilities.spark_worker.json` - Cho Spark Worker
- `worker_capabilities.hadoop_namenode.json` - Cho Hadoop Namenode
- `worker_capabilities.hadoop_datanode.json` - Cho Hadoop Datanode

**VÃ­ dá»¥ ná»™i dung config:**

```json
{
  "worker_name": "spark-master-worker",
  "capabilities": [
    "spark_master",
    "docker_host",
    "prepare_data",
    "train_model",
    "streaming_data",
    "predict"
  ],
  "description": "Spark Master node - handles master services and data pipeline tasks"
}
```

> ğŸ’¡ **Tip**: Sá»­ dá»¥ng automated deployment khi cÃ³ nhiá»u workers Ä‘á»ƒ tiáº¿t kiá»‡m thá»i gian vÃ  trÃ¡nh sai sÃ³t. Chá»‰ cáº§n cáº¥u hÃ¬nh mapping trong `deploy_worker_config.sh` má»™t láº§n, sau Ä‘Ã³ cÃ³ thá»ƒ re-deploy dá»… dÃ ng khi cáº§n update config.

#### BÆ°á»›c 2: Start worker

**CÃ¡ch 1: Sá»­ dá»¥ng script tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)**

```bash
cd ~/Documents/airflow/airflow
./scripts/start_worker.sh
```

Script sáº½:

- âœ… Tá»± Ä‘á»™ng detect capabilities tá»« config file
- âœ… Kiá»ƒm tra káº¿t ná»‘i Redis
- âœ… Subscribe vÃ o cÃ¡c queues phÃ¹ há»£p
- âœ… Start worker vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u

**CÃ¡ch 2: Manual start**

```bash
# Worker tá»± Ä‘á»™ng load config tá»« /etc/celery/worker_capabilities.json
celery -A mycelery.system_worker worker --loglevel=info

# Hoáº·c chá»‰ Ä‘á»‹nh queues thá»§ cÃ´ng
celery -A mycelery.system_worker worker \
  --queues=spark_master,docker_host,prepare_data,train_model \
  --loglevel=info \
  --concurrency=4 \
  --hostname=worker-spark-master@%h
```

#### BÆ°á»›c 3: Verify workers

```bash
# Kiá»ƒm tra workers Ä‘ang hoáº¡t Ä‘á»™ng
celery -A mycelery.system_worker inspect active

# Kiá»ƒm tra queues mÃ  worker subscribe
celery -A mycelery.system_worker inspect active_queues

# Kiá»ƒm tra stats
celery -A mycelery.system_worker inspect stats
```

#### Cáº¥u hÃ¬nh Deployment Script

Äá»ƒ sá»­ dá»¥ng automated deployment, cáº§n cáº¥u hÃ¬nh mapping trong `scripts/deploy_worker_config.sh`:

```bash
# Chá»‰nh sá»­a file scripts/deploy_worker_config.sh

# 1. Cáº­p nháº­t danh sÃ¡ch hosts (IP cá»§a workers)
HOSTS=(
    "192.168.80.55"   # Spark Master
    "192.168.80.53"   # Spark Worker
    "192.168.80.57"   # Hadoop Namenode + Kafka
    "192.168.80.87"   # Hadoop Datanode
)

# 2. Config files tÆ°Æ¡ng á»©ng
CONFIGS=(
    "worker_capabilities.spark_master.json"
    "worker_capabilities.spark_worker.json"
    "worker_capabilities.hadoop_namenode.json"
    "worker_capabilities.hadoop_datanode.json"
)

# 3. SSH username (hoáº·c set env var)
SSH_USER="${SSH_USER:-donghuynh0}"  # Thay báº±ng username cá»§a báº¡n
```

**LÆ°u Ã½:**

- âœ… Thá»© tá»± trong `HOSTS` pháº£i khá»›p vá»›i thá»© tá»± trong `CONFIGS`
- âœ… User cáº§n cÃ³ quyá»n sudo trÃªn cÃ¡c mÃ¡y workers
- âœ… Setup SSH key-based authentication Ä‘á»ƒ trÃ¡nh nháº­p password nhiá»u láº§n:
  ```bash
  # TrÃªn mÃ¡y trung tÃ¢m
  ssh-copy-id user@worker-host
  ```

### Cáº¥u hÃ¬nh nÃ¢ng cao

Worker settings cÃ³ thá»ƒ Ä‘iá»u chá»‰nh qua environment variables:

```bash
export CELERY_WORKER_CONCURRENCY=4          # Sá»‘ tasks cháº¡y Ä‘á»“ng thá»i
export CELERY_WORKER_MAX_TASKS_PER_CHILD=100  # Restart sau N tasks
export CELERY_WORKER_TIME_LIMIT=3600        # Timeout (seconds)
export CELERY_WORKER_LOG_LEVEL=info         # Log level
```

---

## ğŸ”„ Luá»“ng hoáº¡t Ä‘á»™ng

### 1. Kiáº¿n trÃºc thá»±c thi

```
Airflow Scheduler
    â”‚
    â”œâ”€â–º DAG: bigdata_pipeline_start
    â”‚      â”‚
    â”‚      â”œâ”€â–º [PythonOperator] start_hadoop_namenode
    â”‚      â”‚        â””â”€â–º Celery Task â†’ Queue: hadoop_namenode
    â”‚      â”‚                 â””â”€â–º Worker (capability: hadoop_namenode)
    â”‚      â”‚                      â””â”€â–º docker_compose_up(~/bd/hadoop/...)
    â”‚      â”‚
    â”‚      â”œâ”€â–º [PythonOperator] start_spark_master
    â”‚      â”‚        â””â”€â–º Celery Task â†’ Queue: spark_master
    â”‚      â”‚                 â””â”€â–º Worker (capability: spark_master)
    â”‚      â”‚
    â”‚      â””â”€â–º [PythonOperator] prepare_data
    â”‚               â””â”€â–º Celery Task â†’ Queue: prepare_data
    â”‚                        â””â”€â–º Worker (capability: prepare_data)
    â”‚                             â””â”€â–º run_command(sh prepare.sh)
    â”‚
    â””â”€â–º DAG: bigdata_pipeline_stop
           â””â”€â–º [PythonOperator] stop services...
```

### 2. Flow chi tiáº¿t cá»§a má»™t DAG

#### **DAG: bigdata_pipeline_start**

Pipeline nÃ y khá»Ÿi Ä‘á»™ng cluster Big Data vÃ  cháº¡y cÃ¡c tasks xá»­ lÃ½ dá»¯ liá»‡u:

**Phase 1: Infrastructure Setup (Non-blocking)**

```
start_hadoop_namenode  â”€â”€â–º start_hadoop_datanode
                                    â”‚
start_spark_master     â”€â”€â–º start_spark_worker
                                    â”‚
start_kafka â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
                          [Cluster Ready]
```

**Phase 2: Data Processing (Blocking - chá» hoÃ n thÃ nh)**

```
[Cluster Ready]
      â”‚
      â”œâ”€â–º prepare_data (blocking wait)
      â”‚        â”‚
      â”‚        â””â”€â–º Cháº¡y script chuáº©n bá»‹ dá»¯ liá»‡u
      â”‚
      â”œâ”€â–º train_model (blocking wait)
      â”‚        â”‚
      â”‚        â””â”€â–º Train model ML vá»›i Spark
      â”‚
      â”œâ”€â–º predict (parallel vá»›i streaming)
      â”‚        â”‚
      â”‚        â””â”€â–º Cháº¡y dá»± Ä‘oÃ¡n
      â”‚
      â””â”€â–º streaming_data (parallel vá»›i predict)
               â”‚
               â””â”€â–º Stream dá»¯ liá»‡u thá»i gian thá»±c qua Kafka
```

#### **DAG: bigdata_pipeline_stop**

Dá»«ng cluster theo thá»© tá»± an toÃ n:

```
stop_kafka

stop_spark_worker  â”€â”€â–º stop_spark_master

stop_hadoop_datanode  â”€â”€â–º stop_hadoop_namenode
```

### 3. Task Routing Process

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Airflow Task Submit     â”‚
â”‚  op='start_spark_master' â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Celery Task Created     â”‚
â”‚  queue='spark_master'    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Redis Broker            â”‚
â”‚  Add to queue list       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Worker Selection        â”‚
â”‚  Find workers with       â”‚
â”‚  'spark_master' cap      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Task Execution          â”‚
â”‚  docker_compose_up(...)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Result to PostgreSQL    â”‚
â”‚  Airflow tracks status   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4. Task Types

#### **Non-blocking Tasks** (Infrastructure)

- **Má»¥c Ä‘Ã­ch**: Khá»Ÿi Ä‘á»™ng services nhanh, khÃ´ng chá» hoÃ n thÃ nh
- **CÆ¡ cháº¿**: `apply_async()` tráº£ vá» ngay task ID
- **VÃ­ dá»¥**: Start Docker services

```python
result = docker_compose_up.apply_async(
    args=[config['path']],
    kwargs={'detach': True},
    queue='spark_master'  # Route to worker with spark_master capability
)
print(f"Task submitted: {result.id}")
# KhÃ´ng chá», tiáº¿p tá»¥c task tiáº¿p theo
```

#### **Blocking Tasks** (Data Processing)

- **Má»¥c Ä‘Ã­ch**: Pháº£i Ä‘á»£i task hoÃ n thÃ nh má»›i tiáº¿p tá»¥c
- **CÆ¡ cháº¿**: `wait_for_celery_result()` poll cho Ä‘áº¿n khi done
- **VÃ­ dá»¥**: Spark jobs xá»­ lÃ½ dá»¯ liá»‡u

```python
result = run_command.apply_async(
    args=['sh ~/bd/fp_pr_tasks/credit_card/exes/train.sh'],
    queue='train_model'
)
# BLOCKING - Ä‘á»£i Ä‘áº¿n khi task hoÃ n thÃ nh
output = wait_for_celery_result(result, timeout=900)
```

### 5. Configuration Files

#### **DAG Configuration** (`dags/system_control_dag.py`)

Äá»‹nh nghÄ©a services vÃ  mapping capabilities:

```python
BIGDATA_SERVICES = {
    'spark-master': {
        'capability': 'spark_master',
        'path': '~/bd/spark/docker-compose.yml',
        'service': 'spark-master',
    },
    'hadoop-namenode': {
        'capability': 'hadoop_namenode',
        'path': '~/bd/hadoop/docker-compose.namenode.yml',
    },
    # ...
}
```

#### **Worker Configuration** (`mycelery/worker_config.py`)

Map capabilities sang queues:

```python
queue_mapping = {
    'spark_master': ['spark_master', 'spark_common'],
    'prepare_data': ['prepare_data'],
    'train_model': ['train_model'],
    # ...
}
```

#### **Celery Configuration** (`mycelery/celeryconfig.py`)

```python
broker_url = 'redis://192.168.80.229:6379/0'
result_backend = 'db+postgresql://airflow:airflow@192.168.80.229/airflow'
task_time_limit = 3600
worker_concurrency = 4
```

---

## ğŸš€ Sá»­ dá»¥ng

### 1. Truy cáº­p Airflow Web UI

```
http://localhost:9090
```

Login vá»›i credentials máº·c Ä‘á»‹nh:

- **Username**: `airflow`
- **Password**: `airflow`

### 2. Cháº¡y Big Data Pipeline

#### **Start Pipeline**

1. VÃ o DAGs tab â†’ TÃ¬m `bigdata_pipeline_start`
2. Click nÃºt â–¶ï¸ "Trigger DAG"
3. Cáº¥u hÃ¬nh tÃ¹y chá»n (náº¿u cáº§n):
   ```json
   {
     "start_hadoop": true,
     "start_spark": true,
     "start_kafka": true
   }
   ```
4. Click "Trigger"

Pipeline sáº½:

- âœ… Khá»Ÿi Ä‘á»™ng Hadoop cluster (Namenode â†’ Datanode)
- âœ… Khá»Ÿi Ä‘á»™ng Spark cluster (Master â†’ Worker)
- âœ… Khá»Ÿi Ä‘á»™ng Kafka broker
- âœ… Cháº¡y data preparation
- âœ… Train machine learning model
- âœ… Cháº¡y streaming vÃ  prediction song song

#### **Stop Pipeline**

1. VÃ o DAGs tab â†’ TÃ¬m `bigdata_pipeline_stop`
2. Trigger vá»›i options:
   ```json
   {
     "stop_hadoop": true,
     "stop_spark": true,
     "stop_kafka": true,
     "remove_volumes": false
   }
   ```

### 3. Monitoring

#### **Xem logs trong Airflow UI**

1. Click vÃ o DAG run
2. Click vÃ o task cá»¥ thá»ƒ
3. Tab "Logs" hiá»ƒn thá»‹ output chi tiáº¿t

#### **Monitor Celery workers**

```bash
# Xem active tasks
celery -A mycelery.system_worker inspect active

# Xem registered tasks
celery -A mycelery.system_worker inspect registered

# Xem worker stats
celery -A mycelery.system_worker inspect stats
```

#### **Flower UI (Optional)**

Start Flower Ä‘á»ƒ xem Celery dashboard:

```bash
docker compose --profile flower up -d

# Truy cáº­p: http://localhost:5555
```

### 4. Manual Task Execution

CÃ³ thá»ƒ cháº¡y Celery tasks trá»±c tiáº¿p tá»« Python:

```python
from mycelery.system_worker import docker_compose_up, run_command

# Start Spark Master
result = docker_compose_up.apply_async(
    args=['~/bd/spark/docker-compose.yml'],
    kwargs={'services': 'spark-master', 'detach': True},
    queue='spark_master'
)

# Láº¥y káº¿t quáº£
print(result.get(timeout=60))

# Cháº¡y command
result = run_command.apply_async(
    args=['echo "Hello from worker"'],
    queue='spark_master'
)
print(result.get())
```

---

## ğŸ” Troubleshooting

### 1. Worker khÃ´ng nháº­n tasks

**Triá»‡u chá»©ng**: Tasks stuck á»Ÿ tráº¡ng thÃ¡i "queued"

**Kiá»ƒm tra:**

```bash
# 1. Worker cÃ³ Ä‘ang cháº¡y khÃ´ng?
celery -A mycelery.system_worker inspect active_queues

# 2. Config file cÃ³ Ä‘Ãºng khÃ´ng?
cat /etc/celery/worker_capabilities.json

# 3. Redis connection
redis-cli -h 192.168.80.229 ping
```

**Giáº£i phÃ¡p:**

```bash
# Restart worker vá»›i verbose logging
celery -A mycelery.system_worker worker \
  --loglevel=debug \
  --queues=spark_master
```

### 2. Task timeout

**Triá»‡u chá»©ng**: Task bÃ¡o lá»—i "TimeLimitExceeded"

**TÄƒng timeout:**

```python
# Trong DAG
result = run_command.apply_async(
    args=[command],
    kwargs={'env_vars': env_vars},
    queue='train_model',
    time_limit=7200  # 2 hours
)
```

Hoáº·c cáº¥u hÃ¬nh global trong `celeryconfig.py`:

```python
task_time_limit = 7200
task_soft_time_limit = 6600
```

### 3. Airflow khÃ´ng tháº¥y DAGs

**Kiá»ƒm tra:**

```bash
# 1. Xem logs cá»§a dag-processor
docker compose logs airflow-dag-processor

# 2. Verify PYTHONPATH
docker compose exec airflow-scheduler printenv PYTHONPATH
# Should contain: /opt/airflow:/opt/airflow/dags:/opt/airflow/mycelery

# 3. Check DAG syntax
docker compose exec airflow-scheduler \
  python /opt/airflow/dags/system_control_dag.py
```

### 4. Redis connection error

**Lá»—i**: "Error connecting to Redis"

**Kiá»ƒm tra:**

```bash
# 1. Redis cÃ³ cháº¡y khÃ´ng?
docker compose ps redis

# 2. Test connection
redis-cli -h 192.168.80.229 -p 6379 ping

# 3. Check firewall
telnet 192.168.80.229 6379
```

**Cáº­p nháº­t broker URL:**

```bash
# Trong docker-compose.yaml
AIRFLOW__CELERY__BROKER_URL: redis://:@<IP_Má»šI>:6379/0
```

### 5. PostgreSQL connection error

**Kiá»ƒm tra:**

```bash
# Test connection
docker compose exec postgres psql -U airflow -d airflow -c "SELECT 1;"

# Xem logs
docker compose logs postgres
```

### 6. Worker khÃ´ng load capabilities

**Triá»‡u chá»©ng**: Worker chá»‰ subscribe vÃ o queue "celery"

**Debug:**

```bash
# Test worker config
python3 -c "
from mycelery.worker_config import get_worker_capabilities, get_worker_queues
print('Capabilities:', get_worker_capabilities())
print('Queues:', get_worker_queues())
"

# Verify file exists
ls -la /etc/celery/worker_capabilities.json

# Check permissions
sudo chmod 644 /etc/celery/worker_capabilities.json
```

### 7. Docker Compose command fails

**Lá»—i trong logs**: "docker compose up failed"

**Kiá»ƒm tra:**

```bash
# Test command manually trÃªn worker
docker compose -f ~/bd/spark/docker-compose.yml ps

# Verify path exists
ls -la ~/bd/spark/docker-compose.yml

# Check Docker installation
docker --version
docker compose version
```

---

## ğŸ“š TÃ i liá»‡u tham kháº£o

- [Apache Airflow Documentation](https://airflow.apache.org/docs/)
- [Celery Documentation](https://docs.celeryq.dev/)
- [Docker Compose Documentation](https://docs.docker.com/compose/)

---

## ğŸ“ LÆ°u Ã½ quan trá»ng

1. **IP Addresses**: Cáº­p nháº­t Ä‘á»‹a chá»‰ IP trong cÃ¡c file sau cho phÃ¹ há»£p vá»›i mÃ´i trÆ°á»ng cá»§a báº¡n:

   - `docker-compose.yaml`: `AIRFLOW__CELERY__BROKER_URL`
   - `mycelery/system_worker.py`: `REDIS_BROKER`, `CELERY_BACKEND`
   - `mycelery/celeryconfig.py`: `broker_url`, `result_backend`

2. **Security**: Configuration hiá»‡n táº¡i chá»‰ phÃ¹ há»£p cho mÃ´i trÆ°á»ng development. Vá»›i production:

   - Sá»­ dá»¥ng password cho Redis
   - MÃ£ hÃ³a káº¿t ná»‘i database
   - Cáº¥u hÃ¬nh authentication cho Airflow
   - Sá»­ dá»¥ng secrets management

3. **Resource Requirements**:

   - RAM: Tá»‘i thiá»ƒu 4GB cho Airflow cluster
   - CPU: Khuyáº¿n nghá»‹ 2+ cores
   - Disk: Tá»‘i thiá»ƒu 10GB

4. **Backup**: ThÆ°á»ng xuyÃªn backup:
   - PostgreSQL database: `docker-compose exec postgres pg_dump airflow > backup.sql`
   - DAG files: Git repository
   - Worker configs: `/etc/celery/`
